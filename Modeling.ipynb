{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "\n",
    "# data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "# modeling and evaluating\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report,confusion_matrix\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import cross_validate#, metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import GridSearchCV #Perforing grid search\n",
    "\n",
    "# visualizatio\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "\n",
    "\n",
    "data_path='../data/loan.csv' #change to your own folder of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xchen/opt/anaconda3/envs/ds_37/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (19,47,55,112,123,124,125,128,129,130,133,139,140,141) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list with each of the regions by state.\n",
    "\n",
    "west = ['CA', 'OR', 'UT','WA', 'CO', 'NV', 'AK', 'MT', 'HI', 'WY', 'ID']\n",
    "south_west = ['AZ', 'TX', 'NM', 'OK']\n",
    "south_east = ['GA', 'NC', 'VA', 'FL', 'KY', 'SC', 'LA', 'AL', 'WV', 'DC', 'AR', 'DE', 'MS', 'TN' ]\n",
    "mid_west = ['IL', 'MO', 'MN', 'OH', 'WI', 'KS', 'MI', 'SD', 'IA', 'NE', 'IN', 'ND']\n",
    "north_east = ['CT', 'NY', 'PA', 'NJ', 'RI','MA', 'MD', 'VT', 'NH', 'ME']\n",
    "\n",
    "df['region'] = np.nan\n",
    "\n",
    "def finding_regions(state):\n",
    "    if state in west:\n",
    "        return 'West'\n",
    "    elif state in south_west:\n",
    "        return 'SouthWest'\n",
    "    elif state in south_east:\n",
    "        return 'SouthEast'\n",
    "    elif state in mid_west:\n",
    "        return 'MidWest'\n",
    "    elif state in north_east:\n",
    "        return 'NorthEast'\n",
    "    \n",
    "df['region'] = df['addr_state'].apply(finding_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['annual_inc','dti','delinq_2yrs', 'inq_last_6mths', 'total_acc','open_acc','pub_rec']\n",
    "for col in cols:\n",
    "    df[col]=df.groupby(\"region\")[col].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "cols = ['revol_util','mths_since_last_delinq','mths_since_last_record']\n",
    "for col in cols:\n",
    "    df[col] = df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_loan = [\"Charged Off\", \"Default\", \"Does not meet the credit policy. Status:Charged Off\", \"In Grace Period\",\n",
    "            \"Late (16-30 days)\", \"Late (31-120 days)\"]\n",
    "\n",
    "def loan_condition(status):\n",
    "    if status in bad_loan:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['is_bad'] = np.nan\n",
    "df['is_bad'] = df['loan_status'].apply(loan_condition)\n",
    "df.drop(['loan_status'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['is_bad', 'funded_amnt', 'term', 'grade',\n",
    "                 'int_rate', 'installment', 'purpose', 'pymnt_plan', \n",
    "                 'revol_bal','revol_util', \n",
    "                 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_rec_prncp', 'total_rec_int',\n",
    "                 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', \n",
    "                 'home_ownership', 'annual_inc','verification_status', 'addr_state', \n",
    "                 'dti', 'delinq_2yrs', 'inq_last_6mths',\n",
    "                 'total_acc', 'open_acc', 'pub_rec', 'mths_since_last_delinq', 'mths_since_last_record'\n",
    "                 ]\n",
    "df_selected = df[selected_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.isna().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_bad                       int64\n",
       "funded_amnt                  int64\n",
       "term                        object\n",
       "grade                       object\n",
       "int_rate                   float64\n",
       "installment                float64\n",
       "purpose                     object\n",
       "pymnt_plan                  object\n",
       "revol_bal                    int64\n",
       "revol_util                 float64\n",
       "out_prncp                  float64\n",
       "out_prncp_inv              float64\n",
       "total_pymnt                float64\n",
       "total_rec_prncp            float64\n",
       "total_rec_int              float64\n",
       "total_rec_late_fee         float64\n",
       "recoveries                 float64\n",
       "collection_recovery_fee    float64\n",
       "home_ownership              object\n",
       "annual_inc                 float64\n",
       "verification_status         object\n",
       "addr_state                  object\n",
       "dti                        float64\n",
       "delinq_2yrs                float64\n",
       "inq_last_6mths             float64\n",
       "total_acc                  float64\n",
       "open_acc                   float64\n",
       "pub_rec                    float64\n",
       "mths_since_last_delinq     float64\n",
       "mths_since_last_record     float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data is imbalanced and the bad loan is the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260668\n",
      "0    0.868608\n",
      "1    0.131392\n",
      "Name: is_bad, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Number of bad loans\n",
    "print(len(df_selected['is_bad']))\n",
    "# Loan Ratios (Imbalanced classes)\n",
    "print(df_selected['is_bad'].value_counts()/len(df_selected['is_bad']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply StratifiedShuffleSplit to split the dataset into train and test sets randomly ensuring that both sets have the same ration of bad loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set ratio \n",
      " 0    0.868608\n",
      "1    0.131392\n",
      "Name: is_bad, dtype: float64\n",
      "Test set ratio \n",
      " 0    0.868608\n",
      "1    0.131392\n",
      "Name: is_bad, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "stratified = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_set, test_set in stratified.split(df_selected, df_selected[\"is_bad\"]):\n",
    "    stratified_train = df_selected.loc[train_set]\n",
    "    stratified_test = df_selected.loc[test_set]\n",
    "    \n",
    "print('Train set ratio \\n', stratified_train[\"is_bad\"].value_counts()/len(stratified_train))\n",
    "print('Test set ratio \\n', stratified_test[\"is_bad\"].value_counts()/len(stratified_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = stratified_train\n",
    "test_df = stratified_test\n",
    "\n",
    "\n",
    "# Let's Shuffle the data\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Train set (Normal training dataset)\n",
    "X_train = train_df.drop('is_bad', axis=1)\n",
    "y_train = train_df['is_bad']\n",
    "\n",
    "\n",
    "# Test Dataset\n",
    "X_test = test_df.drop('is_bad', axis=1)\n",
    "y_test = test_df['is_bad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode categorical features as a numeric array.\n",
    "    The input to this transformer should be a matrix of integers or strings,\n",
    "    denoting the values taken on by categorical (discrete) features.\n",
    "    The features can be encoded using a one-hot aka one-of-K scheme\n",
    "    (``encoding='onehot'``, the default) or converted to ordinal integers\n",
    "    (``encoding='ordinal'``).\n",
    "    This encoding is needed for feeding categorical data to many scikit-learn\n",
    "    estimators, notably linear models and SVMs with the standard kernels.\n",
    "    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n",
    "        The type of encoding to use (default is 'onehot'):\n",
    "        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n",
    "          (or also called 'dummy' encoding). This creates a binary column for\n",
    "          each category and returns a sparse matrix.\n",
    "        - 'onehot-dense': the same as 'onehot' but returns a dense array\n",
    "          instead of a sparse matrix.\n",
    "        - 'ordinal': encode the features as ordinal integers. This results in\n",
    "          a single column of integers (0 to n_categories - 1) per feature.\n",
    "    categories : 'auto' or a list of lists/arrays of values.\n",
    "        Categories (unique values) per feature:\n",
    "        - 'auto' : Determine categories automatically from the training data.\n",
    "        - list : ``categories[i]`` holds the categories expected in the ith\n",
    "          column. The passed categories are sorted before encoding the data\n",
    "          (used categories can be found in the ``categories_`` attribute).\n",
    "    dtype : number type, default np.float64\n",
    "        Desired dtype of output.\n",
    "    handle_unknown : 'error' (default) or 'ignore'\n",
    "        Whether to raise an error or ignore if a unknown categorical feature is\n",
    "        present during transform (default is to raise). When this is parameter\n",
    "        is set to 'ignore' and an unknown category is encountered during\n",
    "        transform, the resulting one-hot encoded columns for this feature\n",
    "        will be all zeros.\n",
    "        Ignoring unknown categories is not supported for\n",
    "        ``encoding='ordinal'``.\n",
    "    Attributes\n",
    "    ----------\n",
    "    categories_ : list of arrays\n",
    "        The categories of each feature determined during fitting. When\n",
    "        categories were specified manually, this holds the sorted categories\n",
    "        (in order corresponding with output of `transform`).\n",
    "    Examples\n",
    "    --------\n",
    "    Given a dataset with three features and two samples, we let the encoder\n",
    "    find the maximum value per feature and transform the data to a binary\n",
    "    one-hot encoding.\n",
    "    >>> from sklearn.preprocessing import CategoricalEncoder\n",
    "    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n",
    "    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "    ... # doctest: +ELLIPSIS\n",
    "    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n",
    "              encoding='onehot', handle_unknown='ignore')\n",
    "    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n",
    "    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
    "           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "    See also\n",
    "    --------\n",
    "    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n",
    "      integer ordinal features. The ``OneHotEncoder assumes`` that input\n",
    "      features take on values in the range ``[0, max(feature)]`` instead of\n",
    "      using the unique values.\n",
    "    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n",
    "      dictionary items (also handles string-valued features).\n",
    "    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n",
    "      encoding of dictionary items or strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = X_train.select_dtypes(exclude=[\"object\"])\n",
    "categorical = X_train.select_dtypes([\"object\"])\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(numeric.columns.tolist())),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(categorical.columns.tolist())), # We will have to write the categorical columns manually and see if it works.\n",
    "    ('encoder', CategoricalEncoder(encoding=\"onehot-dense\")),\n",
    "])\n",
    "\n",
    "# Combine both Pipelines into one array\n",
    "combined_pipeline = FeatureUnion(transformer_list=[\n",
    "    ('numeric_pipeline', numeric_pipeline),\n",
    "    ('categorical_pipeline', categorical_pipeline)\n",
    "])\n",
    "\n",
    "X_train = combined_pipeline.fit_transform(X_train)\n",
    "X_test = combined_pipeline.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  <class 'numpy.ndarray'> (1808534, 107)\n",
      "Test data:  <class 'numpy.ndarray'> (452134, 107)\n"
     ]
    }
   ],
   "source": [
    "print('Train data: ',type(X_train),X_train.shape)\n",
    "print('Test data: ',type(X_test),X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling the data took: 24.00 minutes!\n"
     ]
    }
   ],
   "source": [
    "# Oversampled Train Set \n",
    "starting_time = time.time()\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=42, n_jobs = -1)\n",
    "X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train)\n",
    "ending_time = time.time()\n",
    "print(\"Resampling the data took: {:.2f} minutes!\".format(round((ending_time - starting_time)/60), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      0, 1570908],\n",
       "       [      1, 1570908]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(np.unique(y_train_sm, return_counts=True)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xchen/opt/anaconda3/envs/ds_37/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.983871\n",
      "Test score: 0.983927\n"
     ]
    }
   ],
   "source": [
    "normal_ypred_lr = lr.predict(X_test)\n",
    "print(\"Train score: %f\" % lr.score(X_train, y_train))\n",
    "print(\"Test score: %f\" % lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC:  0.9794368903427\n",
      "Test AUC:  0.9799586371236347\n"
     ]
    }
   ],
   "source": [
    "normal_pred_lr_train = lr.predict_proba(X_train)[:,1]\n",
    "normal_pred_lr_test = lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('Train AUC: ', roc_auc_score(y_train, normal_pred_lr_train))\n",
    "print('Test AUC: ', roc_auc_score(y_test, normal_pred_lr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Good Loan       0.98      1.00      0.99    392727\n",
      "    Bad Loan       1.00      0.88      0.94     59407\n",
      "\n",
      "    accuracy                           0.98    452134\n",
      "   macro avg       0.99      0.94      0.96    452134\n",
      "weighted avg       0.98      0.98      0.98    452134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Good Loan', 'Bad Loan']\n",
    "print(classification_report(y_test, normal_ypred_lr, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a48b74210>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "normal_lr_cm=confusion_matrix(y_test, normal_ypred_lr)\n",
    "sns.heatmap(normal_lr_cm, annot = True, cmap=\"Greens\",cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> SMOTE Sampling</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xchen/opt/anaconda3/envs/ds_37/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_sm = LogisticRegression()\n",
    "lr_sm.fit(X_train_sm, y_train_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.958065\n",
      "Test score: 0.979382\n"
     ]
    }
   ],
   "source": [
    "smote_ypred_lr = lr_sm.predict(X_test)\n",
    "print(\"Train score: %f\" % lr_sm.score(X_train_sm, y_train_sm))\n",
    "print(\"Test score: %f\" % lr_sm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC:  0.9880314000620045\n",
      "Test AUC:  0.9801059989110401\n"
     ]
    }
   ],
   "source": [
    "smote_pred_lr_train = lr_sm.predict_proba(X_train_sm)[:,1]\n",
    "smote_pred_lr_test = lr_sm.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('Train AUC: ', roc_auc_score(y_train_sm, smote_pred_lr_train))\n",
    "print('Test AUC: ', roc_auc_score(y_test, smote_pred_lr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Good Loan       0.98      0.99      0.99    392727\n",
      "    Bad Loan       0.94      0.90      0.92     59407\n",
      "\n",
      "    accuracy                           0.98    452134\n",
      "   macro avg       0.96      0.94      0.95    452134\n",
      "weighted avg       0.98      0.98      0.98    452134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Good Loan', 'Bad Loan']\n",
    "print(classification_report(y_test, smote_ypred_lr, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a5bc654d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "smote_lr_cm=confusion_matrix(y_test, smote_ypred_lr)\n",
    "sns.heatmap(smote_lr_cm, annot = True, cmap=\"Greens\",cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_lr,tpr_lr,_ = roc_curve(y_test,pred_lr)\n",
    "roc_auc_lr = auc(fpr_lr,tpr_lr)\n",
    "plt.plot(fpr_lr, tpr_lr, linewidth=2, label='logistic regression(auc = {:0.2f})'.format(roc_auc_lr)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(model, Xtrain, ytrain, Xtest, ytest, useTrainCV=True, cv_folds=3, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = model.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(Xtrain, label=ytrain)\n",
    "        cvresult = xgb.cv(xgb_param, \n",
    "                          xgtrain, \n",
    "                          num_boost_round=model.get_params()['n_estimators'], \n",
    "                          nfold=cv_folds,\n",
    "                          metrics='auc', \n",
    "                          early_stopping_rounds=early_stopping_rounds,\n",
    "                          verbose_eval = 20\n",
    "                         )\n",
    "        model.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the model on the data\n",
    "    model.fit(Xtrain, ytrain,eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    xgb_pred_train = model.predict(Xtrain)\n",
    "    xgb_pred_test = model.predict(Xtest)\n",
    "    xgb_predprob_train = model.predict_proba(Xtrain)[:,1]\n",
    "    xgb_predprob_test = model.predict_proba(Xtest)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Accuracy : %.4g\" % accuracy_score(ytrain, xgb_pred))\n",
    "    print (\"Recall (train) : %.4g\" % recall_score(ytrain, xgb_pred_train))\n",
    "    print (\"Recall (test) : %.4g\" % recall_score(ytest, xgb_pred_test))\n",
    "    print (\"AUC Score (Train): %f\" % roc_auc_score(ytrain, xgb_predprob_train))\n",
    "    print (\"AUC Score (Test): %f\" % roc_auc_score(ytest, xgb_predprob_test))\n",
    "                    \n",
    "    feat_imp = pd.Series(model.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Set initial parameters:</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.95587+0.00135\ttest-auc:0.95577+0.00131\n",
      "[20]\ttrain-auc:0.98391+0.00042\ttest-auc:0.98386+0.00046\n",
      "[40]\ttrain-auc:0.99082+0.00027\ttest-auc:0.99077+0.00027\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb1, X_train_sm, y_train_sm, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = {\n",
    "    'learning_rate':0.1,\n",
    "    'max_depth':5,\n",
    "    'min_child_weight':1,\n",
    "    'gamma': 0,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.8,\n",
    "    'objective': 'binary:logistic',\n",
    "    #'silent': 1,\n",
    "    'scale_pos_weight':1,\n",
    "    }\n",
    "Dtrain = xgb.DMatrix(X_train_sm, label=y_train)   \n",
    "cvresult=xgb.cv(params1, Dtrain, num_boost_round=1000, nfold=5, metrics={'auc'},early_stopping_rounds = 50, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> max_depth and min_child_weight</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, \n",
    "                                                  n_estimators=140, \n",
    "                                                  max_depth=5,\n",
    "                                                  min_child_weight=1, \n",
    "                                                  gamma=0, \n",
    "                                                  subsample=0.8, \n",
    "                                                  colsample_bytree=0.8,\n",
    "                                                  objective= 'binary:logistic', \n",
    "                                                  nthread=4, \n",
    "                                                  scale_pos_weight=1, \n",
    "                                                  seed=42), \n",
    "                        param_grid = param_test1, \n",
    "                        scoring='roc_auc',\n",
    "                        n_jobs=-1,\n",
    "                        iid=False, \n",
    "                        cv=5)\n",
    "gsearch1.fit(X_train_sm,y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Gamma</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, \n",
    "                                                  n_estimators=140, \n",
    "                                                  max_depth=4,\n",
    "                                                  min_child_weight=6, \n",
    "                                                  gamma=0, \n",
    "                                                  subsample=0.8, \n",
    "                                                  colsample_bytree=0.8,\n",
    "                                                  objective= 'binary:logistic', \n",
    "                                                  nthread=4, \n",
    "                                                  scale_pos_weight=1,\n",
    "                                                  seed=27\n",
    "                                                 ), \n",
    "                        param_grid = param_test3, \n",
    "                        scoring='roc_auc',\n",
    "                        n_jobs=4,\n",
    "                        iid=False, \n",
    "                        cv=5)\n",
    "gsearch3.fit(train[predictors],train[target])\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
